{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Da best heart disease classifier in town\n",
    "- 13516083 / Abram Perdanaputra\n",
    "- 13516090 / Timothy Thamrin Andrew Hamonangan Sihombing\n",
    "- 13516093 / Muhammad Farhan\n",
    "- 13516153 / Dimas Aditia Pratikto\n",
    "- 13516155 / Restu Wahyu Kartiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import numbers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve function\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to read the `tubes2_HeartDisease_train` and `tubes2_HeartDisease_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data(data):\n",
    "    \"\"\"Convert dataframe to appropriate types\"\"\"\n",
    "    for x in range(1,5):\n",
    "        data.loc[data['Column3'] == x, 'Column3'] = str(x)\n",
    "\n",
    "    num_col = [4, 5, 6, 8, 9, 10, 12]\n",
    "    \n",
    "    for col in num_col:\n",
    "        col_name = 'Column'+str(col)\n",
    "        data[col_name] = pd.to_numeric(data[col_name], errors='coerce')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def show_data(df, columns):\n",
    "    data = copy.deepcopy(df)\n",
    "    data.columns = columns\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data columns and read data from files\n",
    "\n",
    "heart_disease = {}\n",
    "heart_disease['columns_detail'] = [\n",
    "    'Age', \n",
    "    'Sex', \n",
    "    'Pain type', \n",
    "    'Blood pressure', \n",
    "    'Serum cholesterol', \n",
    "    'Fasting blood sugar > 120mg/dl', \n",
    "    'Resting ECG', \n",
    "    'Max heart rate achieved', \n",
    "    'exercise induced agina', \n",
    "    'ST depression induced by exercise relative to rest', \n",
    "    'Peak exercise ST segment', \n",
    "    'Number of major vessels colored by flourosopy', \n",
    "    'Thal', \n",
    "    'Diagnosis'\n",
    "]\n",
    "heart_disease['train'] = pd.read_csv('../data/tubes2_HeartDisease_train.csv')\n",
    "heart_disease['test'] = pd.read_csv('../data/tubes2_HeartDisease_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['train'] = fix_data(heart_disease['train'])\n",
    "show_data(heart_disease['train'], heart_disease['columns_detail']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coba = heart_disease['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocess_data(data):\n",
    "    df = copy.deepcopy(data)\n",
    "    \n",
    "    # remove infinity and null\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # dropping null values\n",
    "    null_array = []\n",
    "    for i, row in df.iterrows():\n",
    "        if check_null(row) > 3:\n",
    "            null_array.append(i)\n",
    "            \n",
    "    null_array.reverse()\n",
    "    for i in null_array:\n",
    "        df = df.drop(df.index[i])\n",
    "\n",
    "    # remove outliers\n",
    "    \n",
    "    # fill null and nan with median\n",
    "    df.loc[data['Column4'].isnull(), 'Column4'] = data['Column4'].median()\n",
    "    df.loc[data['Column5'].isnull(), 'Column5'] = data['Column5'].median()\n",
    "    df.loc[data['Column6'].isnull(), 'Column6'] = data['Column6'].median()\n",
    "    df.loc[data['Column8'].isnull(), 'Column8'] = data['Column8'].median()\n",
    "    df.loc[data['Column9'].isnull(), 'Column9'] = data['Column9'].median()\n",
    "    df.loc[data['Column10'].isnull(), 'Column10'] = data['Column10'].median()\n",
    "    df.loc[data['Column12'].isnull(), 'Column12'] = data['Column12'].median()\n",
    "    \n",
    "    df.loc[np.isnan(data['Column4']), 'Column4'] = data['Column4'].median()\n",
    "    df.loc[np.isnan(data['Column5']), 'Column5'] = data['Column5'].median()\n",
    "    df.loc[np.isnan(data['Column6']), 'Column6'] = data['Column6'].median()\n",
    "    df.loc[np.isnan(data['Column8']), 'Column8'] = data['Column8'].median()\n",
    "    df.loc[np.isnan(data['Column9']), 'Column9'] = data['Column9'].median()\n",
    "    df.loc[np.isnan(data['Column10']), 'Column10'] = data['Column10'].median()\n",
    "    df.loc[np.isnan(data['Column12']), 'Column12'] = data['Column12'].median()\n",
    "    \n",
    "    df.loc[data['Column7'] == '?', 'Column7'] = '0'\n",
    "    df.loc[data['Column11'] == '?', 'Column11'] = '1'\n",
    "    df.loc[data['Column13'] == '?', 'Column11'] = '0'\n",
    "    \n",
    "#     for i, row in df.iterrows():\n",
    "#         if row['Column11'] == '?':\n",
    "#             df.loc[i, 'Column11'] = np.random.choice(['1', '2', '3'], \\\n",
    "#                                                size=1,\\\n",
    "#                                                p=[0.5570599613152805, 0.3404255319148936, 0.10251450676982592])[0]\n",
    "    \n",
    "#     df.loc[:, 'Column1'] = (df['Column1'] - df['Column1'].mean()) / df['Column1'].std()\n",
    "#     df.loc[:, 'Column4'] = (df['Column4'] - df['Column4'].mean()) / df['Column4'].std()\n",
    "#     df.loc[:, 'Column5'] = (df['Column5'] - df['Column5'].mean()) / df['Column5'].std()\n",
    "#     df.loc[:, 'Column8'] = (df['Column8'] - df['Column8'].mean()) / df['Column8'].std()\n",
    "#     df.loc[:, 'Column10'] = (df['Column10'] - df['Column10'].mean()) / df['Column10'].std()\n",
    "    \n",
    "    \n",
    "    # Duplicate targets\n",
    "#     df = df.append([df[df['Column14'] == 4]]*3,ignore_index=True)\n",
    "#     df = df.append([df[df['Column14'] == 3][:12]],ignore_index=True)\n",
    "#     df = df.append([df[df['Column14'] == 2][:12]],ignore_index=True)\n",
    "    \n",
    "    # dropping bad columns\n",
    "    df = df.drop(['Column12', 'Column13'], axis=1)\n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_null(row):\n",
    "    sum = 0\n",
    "    for column in row:\n",
    "        if isinstance(column, numbers.Number) and np.isnan(column):\n",
    "            sum += 1\n",
    "        if not(isinstance(column, numbers.Number)) and column == '?':\n",
    "            sum += 1\n",
    "        if column == None:\n",
    "            sum += 1\n",
    "    return sum\n",
    "\n",
    "def check_outlier(row):\n",
    "    outlier = False\n",
    "    for column in row:\n",
    "        if isinstance(column, numbers.Number) and np.isnan(column):\n",
    "            sum += 1\n",
    "        if not(isinstance(column, numbers.Number)) and column == '?':\n",
    "            sum += 1\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_data(heart_disease['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Distribution on Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Column14'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbor = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=num_neighbor, algorithm='ball_tree')\n",
    "\n",
    "# train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Column14', axis=1), df.Column14,test_size=0.2)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "knn_f1_scores = cross_val_score(knn, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='f1_micro')\n",
    "knn_accuracy_scores = cross_val_score(knn, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='accuracy')\n",
    "print(\"F1 Score: {:.4f} +- {:.4f}\".format(knn_f1_scores.mean(), knn_f1_scores.std()))\n",
    "print(\"Accuracy Score: {:.4f} +- {:.4f}\".format(knn_accuracy_scores.mean(), knn_accuracy_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "# train\n",
    "X_train,X_test,y_train,y_test=X_train, X_test, y_train, y_test = train_test_split(df.drop('Column14', axis=1), df.Column14,test_size=0.2)\n",
    "\n",
    "gnb_f1_scores = cross_val_score(gnb, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='f1_micro')\n",
    "gnb_accuracy_scores = cross_val_score(gnb, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='accuracy')\n",
    "print(\"F1 Score: {:.4f} +- {:.4f}\".format(gnb_f1_scores.mean(), gnb_f1_scores.std()))\n",
    "print(\"Accuracy Score: {:.4f} +- {:.4f}\".format(gnb_accuracy_scores.mean(), gnb_accuracy_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (ID3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "            max_features=8, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=8, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
    "            splitter='best')\n",
    "\n",
    "# train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Column14', axis=1), df.Column14,test_size=0.2)\n",
    "id3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "id3_f1_scores = cross_val_score(id3, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='f1_micro')\n",
    "id3_accuracy_scores = cross_val_score(id3, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='accuracy')\n",
    "print(\"F1 Score: {:.4f} +- {:.4f}\".format(id3_f1_scores.mean(), id3_f1_scores.std()))\n",
    "print(\"Accuracy Score: {:.4f} +- {:.4f}\".format(id3_accuracy_scores.mean(), id3_accuracy_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[3,6,9,12,15], 'min_samples_split':[2,4,8,16], 'min_samples_leaf':[1,2,4,8,16], 'max_features':[2,4,8,10]}\n",
    "grid_search = GridSearchCV(id3, parameters, cv=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(id3.score(X_test,y_test))\n",
    "print(grid_search.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(id3, \"Plot\", df.drop(\"Column14\",axis=1), df['Column14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(id3.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION\n",
    "\n",
    "Metode feature selection yang digunakan adalah Recursive Feature Elemination (RFE). Metode ini mengurutkan atribut-atribut (ranking) dari urutan 1 (paling penting) hingga seterusnya (semakin tidak penting). Disini masi belom sama column12 13 jadi sementara gua delete column4 5 dulu deh katanya kalo ranking ga jelek dihapus jadi ngurangin f1 score nye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df\n",
    "temp = df\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for column in temp:\n",
    "    le.fit(temp[column])\n",
    "    temp[column] = le.transform(temp[column])\n",
    "\n",
    "y = np.array(temp['Column14'])\n",
    "x = np.array(temp.drop(['Column14'], 1))\n",
    "\n",
    "#feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 1)\n",
    "fit = rfe.fit(x, y)\n",
    "print(\"Feature Ranking: \")\n",
    "print(fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.drop(['Column4'], axis=1)\n",
    "# train_df = train_df.drop(['Column5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(train_df['Column14'])\n",
    "train_df['Column14'] = le.transform(train_df['Column14'])\n",
    "\n",
    "train_df = pd.get_dummies(train_df)\n",
    "\n",
    "y = train_df['Column14']\n",
    "x = train_df.drop(['Column14'], 1)\n",
    "\n",
    "# train\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPlearn = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(5, 2), random_state=1)\n",
    "score = cross_val_score(MLPlearn, x, y, cv=10)\n",
    "print(\"F1 Score: {} +- {}\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(14,), random_state=1)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_f1_scores = cross_val_score(mlp, x\\\n",
    "                , y, cv=5, scoring='f1_micro')\n",
    "print(\"F1 Score: {} +- {}\".format(mlp_f1_scores.mean(), mlp_f1_scores.std()))\n",
    "print(mlp.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'hidden_layer_sizes': [(5,), (6,), (7,), (8,), (9,), (10,), (11,), (12,), (13,), (14,), (5, 5), (6, 6), (7, 7), (10, 10), (100,)]}\n",
    "grid_search = GridSearchCV(mlp, parameters, cv=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(mlp.score(X_test,y_test))\n",
    "print(grid_search.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(mlp.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Abram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'solver': ['lbfgs'],\n",
    "    'alpha': [1e-2, 1e-4, 1e-8],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "best_model = GridSearchCV(MLPClassifier(random_state=1, activation='relu'),\n",
    "                         params,\n",
    "                         cv=5,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         verbose=True)\n",
    "\n",
    "best_model.fit(df.drop(\"Column14\",axis=1), df['Column14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(x):\n",
    "    x = np.array(x)\n",
    "    return x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "sds = []\n",
    "models = []\n",
    "for train_index, test_index in kf.split(df):\n",
    "    mlp_abram = MLPClassifier(alpha=0.01,\n",
    "                         hidden_layer_sizes=(100, 100),\n",
    "                         learning_rate='constant',\n",
    "                         solver='lbfgs',\n",
    "                         random_state=1,\n",
    "                         activation='relu')\n",
    "    train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "    mlp_abram.fit(train.drop(['Column14'], axis=1), train['Column14'])\n",
    "    pred = mlp_abram.predict(test.drop(['Column14'], axis=1))\n",
    "    equals = pred == test['Column14']\n",
    "    equals = [int(elem) for elem in equals]\n",
    "    accuracy, sd = get_accuracy(equals)\n",
    "    accuracies.append(accuracy)\n",
    "    sds.append(sd)\n",
    "    models.append(mlp_abram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model: {}\".format(max(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
