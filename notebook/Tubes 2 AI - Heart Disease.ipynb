{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Da best heart disease classifier in town\n",
    "- 13516083 / Abram Perdanaputra\n",
    "- 13516090 / Timothy Thamrin Andrew Hamonangan Sihombing\n",
    "- 13516093 / Muhammad Farhan\n",
    "- 13516153 / Dimas Aditia Pratikto\n",
    "- 13516155 / Restu Wahyu Kartiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to read the `tubes2_HeartDisease_train` and `tubes2_HeartDisease_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data(data):\n",
    "    \"\"\"Convert dataframe to appropriate types\"\"\"\n",
    "    data.loc[data['Column3'] == 1, 'Column3'] = 'typical_agina'\n",
    "    data.loc[data['Column3'] == 2, 'Column3'] = 'atypical_agina'\n",
    "    data.loc[data['Column3'] == 3, 'Column3'] = 'non_aginal_pain'\n",
    "    data.loc[data['Column3'] == 4, 'Column3'] = 'asymtotic'\n",
    "\n",
    "    data.loc[data['Column7'] == '0', 'Column7'] = 'normal'\n",
    "    data.loc[data['Column7'] == '1', 'Column7'] = 'having ST-T wave abnormality'\n",
    "    data.loc[data['Column7'] == '2', 'Column7'] = 'left ventricular hyperthrophy'\n",
    "\n",
    "    data.loc[data['Column11'] == '1', 'Column11'] = 'upsloping'\n",
    "    data.loc[data['Column11'] == '2', 'Column11'] = 'flat'\n",
    "    data.loc[data['Column11'] == '3', 'Column11'] = 'downsloping'\n",
    "\n",
    "    data.loc[data['Column13'] == '3', 'Column13'] = 'normal'\n",
    "    data.loc[data['Column13'] == '6', 'Column13'] = 'fixed_defect'\n",
    "    data.loc[data['Column13'] == '7', 'Column13'] = 'reversable_defect'\n",
    "    \n",
    "    data.Column4 = pd.to_numeric(data.Column4, errors='coerce')\n",
    "    data.Column5 = pd.to_numeric(data.Column5, errors='coerce')\n",
    "    data.Column6 = pd.to_numeric(data.Column6, errors='coerce')\n",
    "    data.Column8 = pd.to_numeric(data.Column8, errors='coerce')\n",
    "    data.Column9 = pd.to_numeric(data.Column9, errors='coerce')\n",
    "    data.Column10 = pd.to_numeric(data.Column10, errors='coerce')\n",
    "    data.Column12 = pd.to_numeric(data.Column12, errors='coerce')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = {}\n",
    "heart_disease['columns_detail'] = [\n",
    "    'Age', \n",
    "    'Sex', \n",
    "    'Pain type', \n",
    "    'Blood pressure', \n",
    "    'Serum cholesterol', \n",
    "    'Fasting blood sugar > 120mg/dl', \n",
    "    'Resting ECG', \n",
    "    'Max heart rate achieved', \n",
    "    'exercise induced agina', \n",
    "    'ST depression induced by exercise relative to rest', \n",
    "    'Peak exercise ST segment', \n",
    "    'Number of major vessels colored by flourosopy', \n",
    "    'Thal', \n",
    "    'Diagnosis'\n",
    "]\n",
    "heart_disease['train'] = pd.read_csv('../data/tubes2_HeartDisease_train.csv')\n",
    "heart_disease['test'] = pd.read_csv('../data/tubes2_HeartDisease_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['train'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocess_data(train_data):\n",
    "    data = copy.deepcopy(train_data)\n",
    "    data.Column4 = pd.to_numeric(data.Column4, errors='coerce')\n",
    "    data.Column5 = pd.to_numeric(data.Column5, errors='coerce')\n",
    "    data.Column6 = pd.to_numeric(data.Column6, errors='coerce')\n",
    "    data.Column8 = pd.to_numeric(data.Column8, errors='coerce')\n",
    "    data.Column9 = pd.to_numeric(data.Column9, errors='coerce')\n",
    "    data.Column10 = pd.to_numeric(data.Column10, errors='coerce')\n",
    "#     data.Column12 = pd.to_numeric(data.Column12, errors='coerce')\n",
    "    \n",
    "    data.loc[data['Column3'] == 1, 'Column3'] = 'typical_agina'\n",
    "    data.loc[data['Column3'] == 2, 'Column3'] = 'atypical_agina'\n",
    "    data.loc[data['Column3'] == 3, 'Column3'] = 'non_aginal_pain'\n",
    "    data.loc[data['Column3'] == 4, 'Column3'] = 'asymtotic'\n",
    "    \n",
    "    data.loc[data['Column4'].isnull(), 'Column4'] = data['Column4'].mean()\n",
    "    data.loc[data['Column5'].isnull(), 'Column5'] = data['Column5'].mean()\n",
    "    data.loc[data['Column6'].isnull(), 'Column6'] = 0\n",
    "\n",
    "    data.loc[data['Column7'] == '0', 'Column7'] = 'normal'\n",
    "    data.loc[data['Column7'] == '1', 'Column7'] = 'having ST-T wave abnormality'\n",
    "    data.loc[data['Column7'] == '2', 'Column7'] = 'left ventricular hyperthrophy'\n",
    "    data.loc[data['Column7'] == '?', 'Column7'] = 'normal'\n",
    "\n",
    "    data.loc[data['Column8'].isnull(), 'Column8'] = 138.348299\n",
    "    data.loc[data['Column9'].isnull(), 'Column9'] = 0.0\n",
    "    data.loc[data['Column10'].isnull(), 'Column10'] = 3.937397\n",
    "\n",
    "    data.loc[data['Column11'] == '1', 'Column11'] = 'upsloping'\n",
    "    data.loc[data['Column11'] == '2', 'Column11'] = 'flat'\n",
    "    data.loc[data['Column11'] == '3', 'Column11'] = 'downsloping'\n",
    "    data.loc[data['Column11'] == '?', 'Column11'] = 'flat'\n",
    "    \n",
    "#     data.loc[data['Column12'].isnull(), 'Column12'] = 0.686792\n",
    "\n",
    "#     data.loc[data['Column13'] == '3', 'Column13'] = 'normal'\n",
    "#     data.loc[data['Column13'] == '6', 'Column13'] = 'fixed_defect'\n",
    "#     data.loc[data['Column13'] == '7', 'Column13'] = 'reversable_defect'\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        if row['Column11'] == '?':\n",
    "            data.loc[i, 'Column11'] = np.random.choice(['upsloping', 'flat', 'downsloping'], \\\n",
    "                                               size=1,\\\n",
    "                                               p=[0.5570599613152805, 0.3404255319148936, 0.10251450676982592])[0]\n",
    "        \n",
    "#         if row['Column13'] == '?':\n",
    "#             data.loc[i, 'Column13'] = np.random.choice(['normal', 'reversable_defect', 'fixed_defect'], \\\n",
    "#                                                size=1,\\\n",
    "#                                                p=[0.46630727762803237, 0.42857142857142855, 0.10512129380053908])[0]\n",
    "#         if np.isnan(row['Column12']):\n",
    "#             data.loc[i, 'Column12'] = np.random.choice([0, 1, 2, 3], \\\n",
    "#                                                size=1,\\\n",
    "#                                                p=[0.5773584905660377, 0.22264150943396227, 0.13584905660377358, 0.06415094339622641])[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['train'].replace([np.inf, -np.inf], np.nan)\n",
    "heart_disease['train'] = preprocess_data(heart_disease['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['train']['Column14'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['train'] = pd.get_dummies(heart_disease['train'])\n",
    "len(heart_disease['train'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbor = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=num_neighbor, algorithm='ball_tree')\n",
    "f1_scores = cross_val_score(KNN, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='f1_micro')\n",
    "accuracy_scores = cross_val_score(KNN, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='accuracy')\n",
    "print(\"F1 Score: {:.4f} +- {:.4f}\".format(f1_scores.mean(), f1_scores.std()))\n",
    "print(\"Accuracy Score: {:.4f} +- {:.4f}\".format(accuracy_scores.mean(), accuracy_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_df = fix_data(pd.read_csv('../data/tubes2_HeartDisease_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "def check_null(row):\n",
    "    sum = 0\n",
    "    for column in row:\n",
    "        if isinstance(column, numbers.Number) and np.isnan(column):\n",
    "            sum += 1\n",
    "        if not(isinstance(column, numbers.Number)) and column == '?':\n",
    "            sum += 1\n",
    "    return sum\n",
    "\n",
    "def check_outlier(row):\n",
    "    outlier = False\n",
    "    for column in row:\n",
    "        if isinstance(column, numbers.Number) and np.isnan(column):\n",
    "            sum += 1\n",
    "        if not(isinstance(column, numbers.Number)) and column == '?':\n",
    "            sum += 1\n",
    "    return sum\n",
    "\n",
    "def nb_preprocess(df):\n",
    "    # dropping bad columns\n",
    "    df = df.drop(['Column13', 'Column12'], axis=1)\n",
    "    \n",
    "    # dropping null values\n",
    "    null_array = []\n",
    "    for i, row in df.iterrows():\n",
    "        if check_null(row) > 4:\n",
    "            null_array.append(i)\n",
    "            \n",
    "    null_array.reverse()\n",
    "    for i in null_array:    \n",
    "        df = df.drop(df.index[i])\n",
    "    \n",
    "    # remove outliers\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nb_preprocess(nb_df)\n",
    "df = pd.get_dummies(preprocess_data(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "df.loc[:, df.columns != 'b']\n",
    "gnb_f1_scores = cross_val_score(gnb, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='f1_micro')\n",
    "gnb_accuracy_scores = cross_val_score(gnb, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='accuracy')\n",
    "print(\"F1 Score: {:.4f} +- {:.4f}\".format(gnb_f1_scores.mean(), gnb_f1_scores.std()))\n",
    "print(\"Accuracy Score: {:.4f} +- {:.4f}\".format(gnb_accuracy_scores.mean(), gnb_accuracy_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (ID3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3_df = fix_data(pd.read_csv('../data/tubes2_HeartDisease_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df = nb_preprocess(id3_df)\n",
    "df = pd.get_dummies(preprocess_data(pp_df))\n",
    "continuous = ['Column1', 'Column4', 'Column5', 'Column8', 'Column10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in continuous:\n",
    "#     df[column+\"_log\"]=np.log(df[column]+np.min(df[column])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "id3 = DecisionTreeRegressor(random_state=rand_state)\n",
    "df.loc[:, df.columns != 'b']\n",
    "id3_f1_scores = cross_val_score(tree, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='f1_micro')\n",
    "id3_accuracy_scores = cross_val_score(id3, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='accuracy')\n",
    "print(\"F1 Score: {:.4f} +- {:.4f}\".format(id3_f1_scores.mean(), id3_f1_scores.std()))\n",
    "print(\"Accuracy Score: {:.4f} +- {:.4f}\".format(id3_accuracy_scores.mean(), id3_accuracy_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly=PolynomialFeatures(2)\n",
    "X_train,X_test,y_train,y_test=train_test_split(poly.fit_transform(df.drop('Column14', axis=1)), df.Column14,test_size=0.2)\n",
    "id3.fit(X_train,y_train)\n",
    "\n",
    "parameters = {'max_depth':[3,6,9,12,15], 'min_samples_split':[2,4,8,16], 'min_samples_leaf':[1,2,4,8,16], 'max_features':[5,10,15]}\n",
    "grid_search = GridSearchCV(id3, parameters, cv=5,verbose=3)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(id3.score(X_test,y_test))\n",
    "print(grid_search.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[3,6,9,12,15], 'min_samples_split':[2,4,8,16], 'min_samples_leaf':[1,2,4,8,16], 'max_features':[10,15,20,25,27]}\n",
    "grid_search = GridSearchCV(id3, parameters, cv=5,verbose=3)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "            max_features=18, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=4, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
    "            splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(tree, \"Plot\", df.drop(\"Column14\",axis=1), df['Column14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly=PolynomialFeatures(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(tree, \"Plot\", poly.fit_transform(df.drop(\"Column14\",axis=1)), df['Column14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"train\"].head()\n",
    "continuous=[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(id3.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "df.loc[:, df.columns != 'b']\n",
    "mlp_f1_scores = cross_val_score(mlp, df.loc[:, df.columns != 'Column14']\\\n",
    "                , df['Column14'], cv=cv, scoring='f1_micro')\n",
    "print(\"F1 Score: {} +- {}\".format(mlp_f1_scores.mean(), mlp_f1_scores.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
